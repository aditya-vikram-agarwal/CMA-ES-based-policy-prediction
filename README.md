1. Predicted 100/100 new policies with CMA-ES convergence that work better than the data generated by a real-life policy with an unknown MDP with real-world application of Reinforcement Learning, like a medical application or business application, where deploying a policy worse than the current one would be dangerous or costly.

2. 100% of the new policies were better than the policy used to generate the sample data for the real-life application MDP. That is, if the new policy is π and the policy used to generate the sample data is πb, then the new policy (all 100 of them) is better: J(π) > J(πb).
